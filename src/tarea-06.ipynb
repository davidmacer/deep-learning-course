{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Deep Learning\n",
    "\n",
    "# CentroGeo\n",
    "\n",
    "## David Mart√≠nez\n",
    "\n",
    "## 18 de junio del 2021"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 1 Solve"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. Consider the unregularized perceptron update for binary classes with learning rate $\\alpha$. Show that using any value of $\\alpha$ is inconsequential in the sense that it only scales up the weight vector by a factor of $\\alpha$. Show that these results also hold true for the multiclass case. Do the results hold true when regularization is used?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "2. Let $(\\bf{x}_i, y_i)$ be a training instance, in which the observed value $y_i$ is predicted from the feature variables $x_i$. Show that the stochastic gradient-descent updates of the perceptron, Widrow-Hoff learning, SVM, and logistic regression are all of the form:\n",
    "\n",
    "    $$\\bf{w} \\leftarrow \\bf{w}(1 - \\alpha \\lambda) + \\alpha y \\left[ \\delta( \\bf{x}, y ) \\right] \\bf{x}$$\n",
    "\n",
    "    Here, the mistake function $\\delta(\\bf{x}, y)$ is $1 - y(\\bf{w}^T \\bf{x})$ for least-squares classification, an indicator variable for perceptron/SVMs, and a probability value for logistic regression. Assume that $\\alpha$ is the learning rate, and $y \\in \\{-1, +1 \\}$. Write the specific forms of $\\delta(\\bf{x}, y) in each case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# 2. Write a code"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "1. The Widrow-Hoff learning rule is also referred to as _Adaline_, which is shortfor adaptive linear neuron. In machine learning, a model combination ensemble averages the scores of multiple models in order to create a more robust classification score. Discuss how you can approximate the averaging of an Adaline and logistic regression with a two-layer neural network. Discuss the similarities and differences of this architecture with an actual model combination ensemble when backpropagation is used to train it. Show how to modify the training process so that the final result is a fine-tuning of the model combination ensemble."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}